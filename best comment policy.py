import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import MultiheadAttention
from agents.utils import batch_to_seq, init_layer, one_hot, run_rnn


class Policy(nn.Module):
    def __init__(self, n_a, n_s, n_step, policy_name, agent_name, identical):
        super(Policy, self).__init__()
        self.name = policy_name
        if agent_name is not None:
            # for multi-agent system
            self.name += '_' + str(agent_name)
        self.n_a = n_a   #number of action
        self.n_s = n_s   #number of state(an agent can observe)
        self.n_step = n_step
        self.identical = identical

    def forward(self, ob, *_args, **_kwargs):
        raise NotImplementedError()

    # init_actor_headÔºåÊòØÁÇ∫‰∫ÜÂàùÂßãÂåñ‰∏ÄÂÄãactorÈÄôÂÄãÈ†≠ÁöÑÂàùÂßãÁãÄÊÖãÔºåÊúÄ‰∏ÄÈñãÂßãÊúÉÂæûhidden layerÂåØÂÖ•ÈÄ≤‰æÜÔºåÈÇ£hidden layerÂâáÂ∞±ÊúÉË∑üaction layer
    # ÂΩ¢Êàê‰∏ÄÂÄãÂÖ®ÈÄ£Êé•Â±§Ôºå
    def _init_actor_head(self, n_h, n_a=None):
        if n_a is None:
            n_a = self.n_a
        # hiddenËàáactionÂ±§ÈÄ£Êé•
        self.actor_head = nn.Linear(n_h, n_a)
        init_layer(self.actor_head, 'fc')
    # critic head

    # initializes a neural network layer (critic head) for estimating the value function.
    # ‰ΩúÁî®Â∞±ÊòØ  Ë®≠ÂÆöÁ•ûÁ∂ìÁ∂≤Ë∑ØÁµêÊßãÁöÑÂèÉÊï∏Ôºàhidden layer Á∂≠Â∫¶Á≠âÔºâ„ÄÇ
    # Ëº∏Âá∫	Ê≤íÊúâÁõ¥Êé•Ëº∏Âá∫ÔºåÂè™ÂàùÂßãÂåñÁ•ûÁ∂ìÁ∂≤Ë∑ØÂ±§„ÄÇ
    # ‰ΩúÁî®	ÂâµÂª∫ critic_headÔºàÊ®°ÂûãÁµêÊßãÔºâ
    def _init_critic_head(self, n_h, n_n=None):
        if n_n is None:
            n_n = int(self.n_n)   # n_n Â∞±ÊòØnumber of neighbors
        if n_n:
            # ÈÄôÈÇäË¶ÅÁúãÊúâÊ≤íÊúâidenticalÔºåÂ¶ÇÊûúÂ§ßÂÆ∂ÈÉΩ‰∏ÄÊ®£ÔºåÈÇ£Âü∫Êú¨‰∏äÂ∞±Áõ¥Êé•action dimension * ÊúâÂπæÂÄãneighborÂ∞±ÊòØÁ∏ΩdimensionÊï∏Èáè  # Ë¶ÅË®àÁÆón_na_sparse Â∞±ÊòØÂõ†ÁÇ∫criticÊúÄÂæåÊòØÊúÉÂèÉÁÖßÊâÄÊúâagentÁöÑ
            if self.identical:
                n_na_sparse = self.n_a*n_n
            # È∫ªÁÖ©ÁöÑÁ∏ΩÊòØÊ≤íÊúâidenticalÔºåÊúâ‰∫∫‰∏ÄÊ®£Êúâ‰∫∫‰∏ç‰∏ÄÊ®£„ÄÇË®òÂæóÂóé? na_dim_lsÊòØ‰∏ÄÂÄãlistÔºå‰ªñÊúÉÂ≠òÈÄôÂÄãagentÁöÑÈÑ∞Â±ÖÁöÑactionÁ∂≠Â∫¶ÔºåÊâÄ‰ª•ÊääÂÆÉ
            # sumËµ∑‰æÜÂ∞±ÊòØÈÑ∞Â±ÖactionÁ∏ΩÁ∂≠Â∫¶ÔºåÂ•ΩÊ£íÂë¶
            else:
                n_na_sparse = sum(self.na_dim_ls)
            # ÊâÄ‰ª•ÈÑ∞Â±ÖÁöÑactionÁ∏ΩÁ∂≠Â∫¶‰πüÊúÉÁï∂Êàêhidden layer, ÊâÄ‰ª•Ë¶ÅÂä†ÈÄ≤hiddn layerÁöÑdimensionÁï∂‰∏≠
            n_h += n_na_sparse
        # ÈÄôÂÄãÂú∞ÊñπÂèØ‰ª•ÁúãÂà∞Ôºå‰ªñÊòØ‰∏ÄÂÄãÁ∑öÊÄßÂ±§ÁÑ∂ÂæåÊòØÂæûÂâõÂâõÁöÑhidden dimensionÂà∞1
        # BUT....ÁÇ∫‰ΩïÊòØ1?
        # ÂÆöÁæ© critic_head (class attribute)
        self.critic_head = nn.Linear(n_h, 1)
        init_layer(self.critic_head, 'fc')

    # calculates the critic‚Äôs output given inputs (hidden state h and neighbor actions na
    # Ëº∏ÂÖ•  Á•ûÁ∂ìÁ∂≤Ë∑ØÁöÑËº∏ÂÖ•Ë≥áÊñô(LSTMÁöÑËº∏Âá∫) ÂíåÈÑ∞Â±Ö‰ª•ÂèäËá™Â∑±ÁöÑaction
    # Ëº∏Âá∫  È†êÊ∏¨Á¥ØÁ©çÁçéÂãµÁöÑ scalar ÂÄº
    # ‰ΩúÁî®  ÊäähÈÄÅÈÄ≤critic_head ÂÅöÈÅãÁÆó‰∏¶Ëº∏Âá∫È†êÊúüÁçéÂãµ
    # h Â∞±ÊòØagentËá™Â∑±ÁöÑLSTM outputÔºånaÊòØÈÑ∞Â±ÖÈ†êË®àÊé°ÂèñÁöÑaction
    def _run_critic_head(self, h, na, n_n=None):
        if n_n is None:
            n_n = int(self.n_n)
        if n_n:
            na = torch.from_numpy(na).long()  # naÊú¨‰æÜÊòØnumpy arrayÔºåÊàëÂÄëÊää‰ªñËΩâÊàêtensorÂΩ¢Âºè
            if self.identical:
                # ÊääÈÑ∞Â±ÖÁöÑactionÂãï‰ΩúÈÄ≤Ë°åone-hotÁ∑®Á¢º‰πãÂæåÊî§Âπ≥
                na_sparse = one_hot(na, self.n_a)
                na_sparse = na_sparse.view(-1, self.n_a*n_n)
            else:
                # Â¶ÇÊûú‰∏çidenticalÔºåÈÑ∞Â±ÖÁöÑaction dimension‰∏çÂêåÔºåÂ∞±ÊúÉË∂ÖÈ∫ªÁÖ©
                # Áî®ÊñáÂ≠ó‰∏çÂ•ΩË°®Á§∫ÔºåÊàëÂ∞áGPTÁöÑÂõûÁ≠îÊîæhttps://drive.google.com/file/d/1BJF7NxvZvSun4ywGVqCFzNuRU8BXxXjj/view?usp=drive_link
                # ÂèØËá™Ë°åÊê≠ÈÖçÁ®ãÂºèÁêÜËß£
                # Neighbor actions (na) are one-hot encoded and concatenated:
                # ùëõùëé_ùë†ùëùùëéùëüùë†ùëí=[one-hot(Neighbor¬†1)]+[one-hot(Neighbor¬†2)]   ### Combine h (agent's hidden layer output) and na_sparse (neighbor actions).
                na_sparse = []
                na_ls = torch.chunk(na, n_n, dim=1)
                for na_val, na_dim in zip(na_ls, self.na_dim_ls):
                    na_sparse.append(torch.squeeze(
                        one_hot(na_val, na_dim), dim=1))
                na_sparse = torch.cat(na_sparse, dim=1)

            h = torch.cat([h, na_sparse.cuda()], dim=1)
        # The critic_head(h) computes the final scalar output, which is the predicted cumulative reward.
        return self.critic_head(h).squeeze()

    def _run_loss(self, actor_dist, e_coef, v_coef, vs, As, Rs, Advs):
        # ÈÄôÂù®Âü∫Êú¨‰∏äÂ∞±ÊòØÂÅöÂá∫Á¢©Ë´ñpp31pp32ÁöÑÊºîÁÆóÊ≥ï3.3&3.4
        # ‰∏ªË¶ÅÊúâ‰∏âÁ®ÆlossÔºåactor„ÄÅÊ≠£ÂâáÈ†Ö„ÄÅcriticÁöÑloss

        # ÂèØËÉΩÈúÄË¶ÅÁöÑË≥áË®äÊúâÔºö
        # actor_distÔºöÂãï‰ΩúÁöÑÊ¶ÇÁéáÂàÜ‰Ωà(policy œÄ) œÄ(a‚à£s)
        # e_coefÔºöÁÜµÊêçÂ§±ÁöÑÊ¨äÈáç‰øÇÊï∏
        # v_coefÔºöÂÉπÂÄºÊêçÂ§±ÁöÑÊ¨äÈáç‰øÇÊï∏
        # vsÔºöÂÉπÂÄº‰º∞Ë®à
        # AsÔºöÂØ¶ÈöõÊé°ÂèñÁöÑÂãï‰Ωú
        # RsÔºöÁõÆÊ®ôÂÉπÂÄºÔºàÂõûÂ†±Ôºâ
        # AdvsÔºöÂÑ™Âã¢ÂáΩÊï∏  ‚ãÖA(s,a)
        As = As.cuda()
        Advs = Advs.cuda()
        Rs = Rs.cuda()

        # ÈÄôÂÖ©Ë°åÂ∞±ÊòØactorÁöÑloss(policy loss)
        # Lpolicy = ‚àíE[logœÄ(a‚à£s)‚ãÖA(s,a)]  -----3.3 ÂâçÈù¢ÈÇ£‰∏ÄÈ†Ö

        log_probs = actor_dist.log_prob(As)  # logœÄ(a‚à£s)
        policy_loss = -(log_probs * Advs).mean()  # A(s,a)

        # Entropy Loss  Ê≠£ÂâáÈ†ÖÔºåÂ¢ûÂä†Á≠ñÁï•ÁöÑÊé¢Á¥¢ÊÄßÔºåÈò≤Ê≠¢Á≠ñÁï•ÈÅéÊó©Èô∑ÂÖ•Â±ÄÈÉ®ÊúÄÂÑ™  -----3.3ÂæåÈù¢ÈÇ£‰∏ÄÈ†Ö
        entropy_loss = -(actor_dist.entropy()).mean() * e_coef

        # L value=E[(R‚àíV(s))2] ----3.4
        value_loss = (Rs - vs).pow(2).mean() * v_coef

        # ÈÄôË£°ÁöÑ e_coef Âíå v_coef ÊòØÁî®‰æÜË™øÊï¥ÁÜµÊêçÂ§±ÂíåÂÉπÂÄºÊêçÂ§±Â∞çÁ∏ΩÊêçÂ§±ÁöÑÂΩ±ÈüøÁ®ãÂ∫¶„ÄÇ
        return policy_loss, value_loss, entropy_loss

    def _update_tensorboard(self, summary_writer, global_step):
        # monitor training
        # ÈÄôÊàëÂÖàË∑≥ÈÅé
        summary_writer.add_scalar('loss/{}_entropy_loss'.format(self.name), self.entropy_loss,
                                  global_step=global_step)
        summary_writer.add_scalar('loss/{}_policy_loss'.format(self.name), self.policy_loss,
                                  global_step=global_step)
        summary_writer.add_scalar('loss/{}_value_loss'.format(self.name), self.value_loss,
                                  global_step=global_step)
        summary_writer.add_scalar('loss/{}_total_loss'.format(self.name), self.loss,
                                  global_step=global_step)


class LstmPolicy(Policy):
    def __init__(self, n_s, n_a, n_n, n_step, n_fc=64, n_lstm=64, name=None,
                 na_dim_ls=None, identical=True):
        super(LstmPolicy, self).__init__(
            n_a, n_s, n_step, 'lstm', name, identical)
        if not self.identical:
            self.na_dim_ls = na_dim_ls
        #
        # n_fcÔºöÂÖ®ÈÄ£Êé•Â±§ÁöÑÂñÆÂÖÉÊï∏
            # n_lstmÔºöLSTM Â±§ÁöÑÂñÆÂÖÉÊï∏
        self.n_lstm = n_lstm
        self.n_fc = n_fc
        self.n_n = n_n
        self._init_net()
        self._reset()

    def backward(self, obs, nactions, acts, dones, Rs, Advs,
                 e_coef, v_coef, summary_writer=None, global_step=None):
        obs = torch.from_numpy(obs).float()
        dones = torch.from_numpy(dones).float()
        obs = obs.cuda()
        dones = dones.cuda()
        xs = self._encode_ob(obs)
        hs, new_states = run_rnn(self.lstm_layer, xs, dones, self.states_bw)

        # ‰∏ãÈù¢ÊòØÂú®Ë®àÁÆólossÔºå
        # backward grad is limited to the minibatch
        self.states_bw = new_states.detach()
        actor_dist = torch.distributions.categorical.Categorical(
            logits=F.log_softmax(self.actor_head(hs), dim=1))  # hs‰∏üÂÖ•actor_headÔºåÂÜçÁ∂ìÈÅésoft max
        vs = self._run_critic_head(hs, nactions)  # hs ‰ª•ÂèäËá™Â∑±ËàáÈÑ∞Â±ÖÁöÑÂãï‰Ωú‰∏üÂÖ•critic_head
        self.policy_loss, self.value_loss, self.entropy_loss = \
            self._run_loss(actor_dist, e_coef, v_coef, vs,
                           torch.from_numpy(acts).long(),
                           torch.from_numpy(Rs).float(),
                           torch.from_numpy(Advs).float())
        self.loss = self.policy_loss + self.value_loss + \
            self.entropy_loss  # Á∏ΩÂÖ±ÁöÑlossÔºåÁ∂ìÈÅérun_loss‰πãÂæåÂæóÂà∞‰∏âÁ®ÆlossÔºåÂÜçÊää‰ªñÂÄëÂä†Ëµ∑‰æÜ
        self.loss.backward()  # Ë®àÁÆógradientÔºåÂè™Â∑ÆÊ≤íÊúâoptimizer.step() Êõ¥Êñ∞NNÂèÉÊï∏
        if summary_writer is not None:
            self._update_tensorboard(summary_writer, global_step)

    def forward(self, ob, done, naction=None, out_type='p'):
        ob = torch.from_numpy(np.expand_dims(ob, axis=0)).float().cuda()
        done = torch.from_numpy(np.expand_dims(done, axis=0)).float().cuda()
        x = self._encode_ob(ob)  # fc
        h, new_states = run_rnn(
            self.lstm_layer, x, done, self.states_fw)  # lstm
        if out_type.startswith('p'):
            self.states_fw = new_states.detach()
            # actor head
            return F.softmax(self.actor_head(h), dim=1).squeeze().cpu().detach().numpy()
        else:
            # critic head
            return self._run_critic_head(h, np.array([naction])).cpu().detach().numpy()

    def _encode_ob(self, ob):
        # fc_layer ÊòØÂú®‰∏ãÈù¢init_net(self)‰∏≠ÂÆöÁæ©ÁöÑÔºå‰∏ãÈù¢ÂÆöÁæ©(ÂàùÂßãÂåñ)‰∫ÜÂêÑÁ®ÆÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÊØîÂ¶ÇË™™Ë¶ÅÂ§öÂ∞ëneauronÊãâÔºåËº∏ÂÖ•Á∂≠Â∫¶Êãâ
        return F.relu(self.fc_layer(ob))

    def _init_net(self):
        # ÈÄôÈÇäÂÜçÁµ±Êï¥‰∏Ä‰∏ãÊ®°Âûã‰∏≠Áî®Âà∞ÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑Ø
        # 1. ÂÖ®ÈÄ£Êé•Â±§fc_layer: Áî®ÊñºÊäähidden_layerÊäïÂ∞ÑÂà∞ÁâπÂæµÁ©∫Èñì
        # 2. LSTMÂ∞±ÊòØLSTM
        # 3. Á≠ñÁï•È†≠ÔºåË™™Âà∞ÈÄôÂÄãÔºåË¨õ‰∏Ä‰∏ãÔºå„ÄåÈ†≠„ÄçÊòØÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁöÑÂ∞àÊ•≠Áî®Ë™ûÔºåÈÄôÂÄãÁ∂≤Ë∑ØÊúâË∫´È´îÔºåÁÑ∂Âæå‰ªñÁöÑËº∏Âá∫Á´ØÊúâÂÖ©ÂÄãÔºåÂ∞±ÊòØÂÖ©ÂÄã„ÄåÈ†≠„Äç
        #    Á≠ñÁï•È†≠ÁöÑÁõÆÁöÑÂú®ÊñºËº∏Âá∫Âãï‰ΩúÁöÑÊ©üÁéáÂàÜÂ∏É
        # 4. Ë©ïË´ñÈ†≠ÔºåË©ïË´ñÈ†≠Â∞±ÊòØÊúÉËº∏Âá∫ÈÄôÂÄã...value
        self.fc_layer = nn.Linear(self.n_s, self.n_fc)
        init_layer(self.fc_layer, 'fc')
        self.lstm_layer = nn.LSTMCell(self.n_fc, self.n_lstm)
        init_layer(self.lstm_layer, 'lstm')
        self._init_actor_head(self.n_lstm)
        self._init_critic_head(self.n_lstm)

    def _reset(self):
        # forget the cumulative states every cum_step
        self.states_fw = torch.zeros(self.n_lstm * 2)
        self.states_bw = torch.zeros(self.n_lstm * 2)





class NCMultiAgentPolicy(Policy):
    def __init__(self, n_s, n_a, n_agent, n_step, neighbor_mask, n_fc=64, n_h=64,
                 n_s_ls=None, n_a_ls=None, identical=True):
        super(NCMultiAgentPolicy, self).__init__(
            n_a, n_s, n_step, 'nc', None, identical)
        if not self.identical:
            self.n_s_ls = n_s_ls
            self.n_a_ls = n_a_ls
        self.n_agent = n_agent
        self.neighbor_mask = neighbor_mask
        self.n_fc = n_fc
        self.n_h = n_h
        self._init_net()
        self._reset()

        # Initialize Separate Self-Attention Layers for Each Agent
        self.attention_layers = nn.ModuleList()
        for _ in range(self.n_agent):
            attention_layer = nn.MultiheadAttention(
                embed_dim=self.n_h, num_heads=1)
            init_layer(attention_layer, 'fc')  # Ensure proper initialization
            self.attention_layers.append(attention_layer)

    def backward(self, obs, fps, acts, dones, Rs, Advs,
                 e_coef, v_coef, summary_writer=None, global_step=None):
        obs = torch.from_numpy(obs).float().transpose(0, 1)
        dones = torch.from_numpy(dones).float()
        fps = torch.from_numpy(fps).float().transpose(0, 1)
        acts = torch.from_numpy(acts).long()
        hs, new_states = self._run_comm_layers(obs, dones, fps, self.states_bw)
        # backward grad is limited to the minibatch
        self.states_bw = new_states.detach()
        ps = self._run_actor_heads(hs)
        vs = self._run_critic_heads(hs, acts)
        self.policy_loss = 0
        self.value_loss = 0
        self.entropy_loss = 0
        Rs = torch.from_numpy(Rs).float()
        Advs = torch.from_numpy(Advs).float()
        for i in range(self.n_agent):
            actor_dist_i = torch.distributions.categorical.Categorical(
                logits=ps[i])
            policy_loss_i, value_loss_i, entropy_loss_i = \
                self._run_loss(actor_dist_i, e_coef, v_coef, vs[i],
                               acts[i], Rs[i], Advs[i])
            self.policy_loss += policy_loss_i
            self.value_loss += value_loss_i
            self.entropy_loss += entropy_loss_i
        self.loss = self.policy_loss + self.value_loss + self.entropy_loss
        self.loss.backward()
        if summary_writer is not None:
            self._update_tensorboard(summary_writer, global_step)

    def forward(self, ob, done, fp, action=None, out_type='p'):
        # TxNxm
        ob = torch.from_numpy(np.expand_dims(ob, axis=0)).float()
        done = torch.from_numpy(np.expand_dims(done, axis=0)).float()
        fp = torch.from_numpy(np.expand_dims(fp, axis=0)).float()
        # h dim: NxTxm
        h, new_states = self._run_comm_layers(ob, done, fp, self.states_fw)
        if out_type.startswith('p'):
            self.states_fw = new_states.detach()
            return self._run_actor_heads(h, detach=True)
        else:
            action = torch.from_numpy(np.expand_dims(action, axis=1)).long()
            return self._run_critic_heads(h, action, detach=True)

# ‰∏çÁî®Êîπ
    def _get_comm_s(self, i, n_n, x, h, p):
        # ÈÄômethod‰∏ªË¶ÅÈáùÂ∞çÂñÆ‰∏ÄagentÁöÑLSTMÁöÑËº∏ÂÖ•ÂÅöËôïÁêÜÔºåÂÖ∑È´îÂÖßÂÆπÂ¶Ç‰∏ã
        # 1.Êî∂ÈõÜÈÑ∞Â±ÖÁöÑË≥áË®äÔºàËßÄÊ∏¨ÂÄº„ÄÅÈö±ËóèÁãÄÊÖã„ÄÅÁ≠ñÁï•ÊåáÁ¥ãÔºâ
        # 2.Â∞çÈÄô‰∫õË≥áË®äÈÄ≤Ë°åËôïÁêÜ
        # 3.ÈÄöÈÅéÂÖ®ÈÄ£Êé•Â±§Ôºàfc_x_layers„ÄÅfc_p_layers„ÄÅfc_m_layersÔºâËôïÁêÜÔºå‰∏¶ÊáâÁî® ReLU ÊøÄÊ¥ªÂáΩÊï∏
        # 4.Â∞áËôïÁêÜÂæåÁöÑÁâπÂæµÊãºÊé•Ôºå‰ΩúÁÇ∫ LSTM ÁöÑËº∏ÂÖ•

        
        h = h.cuda()    # hÊòØÊâÄÊúâagentÁöÑhidden state(ÊâÄÊúâ LSTM layers t-1ÊôÇÂàªÁöÑÁãÄÊÖã) have a fixed size (n_h) for all agents
        x = x.cuda()    # xÊòØÊâÄÊúâagentÁõÆÂâçÁöÑËßÄÂØüÂÄº (ob)
        p = p.cuda()    # pÊòØÊâÄÊúâagentÁöÑpolicy fingerprint(t-1ÊôÇÂàªÁöÑÊâÄÊúâactor_headËº∏Âá∫)
        ########################################################################################ÂâçÈù¢h,x,pÈÉΩÊòØÊâÄÊúâagentÁöÑÔºåÁèæÂú®Ë¶ÅÊì∑ÂèñÈÑ∞Â±ÖÁöÑ
        # jsÊòØÁõÆÂâçagnetÁöÑÈÑ∞Â±ÖÁöÑindexÔºåFor example, if self.neighbor_mask[i] = [0, 1, 0, 1], it means agents 1 and 3 are neighbors of agent i.
        js = torch.from_numpy(np.where(self.neighbor_mask[i])[0]).long().cuda()   #jsÊääÁõÆÂâçagentÁöÑÈÑ∞Â±ÖÊòØÊúâË™∞ÔºåÁî®indexÂ≠òÊàê‰∏ÄÂÄãlist
        
        m_i = torch.index_select(h, 0, js)   ###Ê†πÊìöÂâõÂâõÂæóÂà∞ÁöÑÈÑ∞Â±Öindex(js)ÔºåÁèæÂú®ÂæûÂÖ®ÈÉ®ÁöÑhidden state(h)‰∏≠Ë¶ÅÁØ©ÈÅ∏Âá∫ÈÑ∞Â±ÖÁöÑhidden stateÔºå‰ΩúÁÇ∫FC_m_layersÁöÑËº∏ÂÖ•(m_i)
        p_i = torch.index_select(p, 0, js)   # p_i ÊòØÈÑ∞Â±ÖÂú®t-1ÊôÇÁöÑpolicy fingerprint
        nx_i = torch.index_select(x, 0, js)  # nx_i ÊòØÈÑ∞Â±ÖÂú®tÊôÇÂàªÁöÑËßÄÊ∏¨ÂÄº

        m_i = m_i.view(1, self.n_h * n_n)    ###ÈÄôË£°Áõ¥Êé•ÊääÈÑ∞Â±ÖÁöÑhidden_stateÔºåÂ±ïÂπ≥Êàê‰∏ÄÁ∂≠ÂêëÈáèÔºåÂΩ¢ÁãÄÁÇ∫ (1, self.n_h * n_n)„ÄÇÂõ†ÁÇ∫LSTMÁöÑoutputÊâÄÊúâagentÈÉΩ‰∏ÄÊ®£Ôºå‰∏çÂàÜidentical or not

        if self.identical:
            p_i = p_i.view(1, self.n_a * n_n)
            nx_i = nx_i.view(1, self.n_s * n_n)
            x_i = x[i].unsqueeze(0)
        else:
            p_i_ls = []  # ÈÄôÂÄãlistÁî®‰æÜÂ≠òÂÄãÈÑ∞Â±ÖÁöÑfingerprint
            nx_i_ls = []  # ÈÄôÂÄãÂâáÊòØÈÑ∞Â±ÖÁöÑobservation
            for j in range(n_n):
                p_i_ls.append(p_i[j].narrow(0, 0, self.na_ls_ls[i][j]))
                nx_i_ls.append(nx_i[j].narrow(0, 0, self.ns_ls_ls[i][j]))
            p_i = torch.cat(p_i_ls).unsqueeze(0)
            nx_i = torch.cat(nx_i_ls).unsqueeze(0)
            x_i = x[i].narrow(0, 0, self.n_s_ls[i]).unsqueeze(0)
        s_i = [F.relu(self.fc_x_layers[i](torch.cat([x_i, nx_i], dim=1))),  # Á¨¨‰∏ÄÈ†ÖËº∏ÂÖ•ÔºåËá™Â∑±ËàáÈÑ∞Â±ÖÁöÑËßÄÂØü„ÄÇÈÑ∞Â±ÖÁöÑËßÄÊ∏¨ÂÄºÔºànx_i)„ÄÇÁï∂Ââç‰ª£ÁêÜÁöÑËßÄÊ∏¨ÂÄºÔºàx_i)
               # ÈÑ∞Â±ÖÁ≠ñÁï•ÊëòË¶ÅÔºàp_iÔºâÁ∂ì fc_p_layer ËôïÁêÜÂæåÔºåÊàêÁÇ∫Áï∂Ââç‰ª£ÁêÜÁöÑËº∏ÂÖ•ÁâπÂæµ‰πã‰∏Ä Á¨¨‰∏âÈ†ÖËº∏ÂÖ•
               F.relu(self.fc_p_layers[i](p_i)),
               F.relu(self.fc_m_layers[i](m_i))]  # Á¨¨‰∫åÈ†ÖËº∏ÂÖ•
        return torch.cat(s_i, dim=1)  # si ÊòØLSTMÁöÑËº∏ÂÖ•

    def _get_neighbor_dim(self, i_agent):
        n_n = int(np.sum(self.neighbor_mask[i_agent]))
        if self.identical:
            return n_n, self.n_s * (n_n+1), self.n_a * n_n, [self.n_s] * n_n, [self.n_a] * n_n
        else:
            ns_ls = []
            na_ls = []
            for j in np.where(self.neighbor_mask[i_agent])[0]:
                ns_ls.append(self.n_s_ls[j])
                na_ls.append(self.n_a_ls[j])
            return n_n, self.n_s_ls[i_agent] + sum(ns_ls), sum(na_ls), ns_ls, na_ls

    def _init_actor_head(self, n_a):
        # only discrete control is supported for now
        actor_head = nn.Linear(self.n_h, n_a)
        init_layer(actor_head, 'fc')
        self.actor_heads.append(actor_head)

    def _init_comm_layer(self, n_n, n_ns, n_na):
        ''' 
        n_n: Number of neighbors.
        n_ns: Combined state dimension of the agent and its neighbors.
        n_na: Combined action dimension of the neighbors.
        '''
        n_lstm_in = 3 * self.n_fc
        fc_x_layer = nn.Linear(n_ns, self.n_fc)
        init_layer(fc_x_layer, 'fc')
        self.fc_x_layers.append(fc_x_layer)
        if n_n:
            fc_p_layer = nn.Linear(n_na, self.n_fc)
            init_layer(fc_p_layer, 'fc')
            fc_m_layer = nn.Linear(self.n_h * n_n, self.n_fc)
            init_layer(fc_m_layer, 'fc')
            self.fc_m_layers.append(fc_m_layer)
            self.fc_p_layers.append(fc_p_layer)
            lstm_layer = nn.LSTMCell(n_lstm_in, self.n_h)
        else:
            self.fc_m_layers.append(None)
            self.fc_p_layers.append(None)
            lstm_layer = nn.LSTMCell(self.n_fc, self.n_h)
        init_layer(lstm_layer, 'lstm')
        self.lstm_layers.append(lstm_layer)

    def _init_critic_head(self, n_na):
        critic_head = nn.Linear(self.n_h + n_na, 1)
        init_layer(critic_head, 'fc')
        self.critic_heads.append(critic_head)

    def _init_net(self):
        # Initialize existing layers
        self.fc_x_layers = nn.ModuleList()
        self.fc_p_layers = nn.ModuleList()
        self.fc_m_layers = nn.ModuleList()
        self.lstm_layers = nn.ModuleList()
        self.actor_heads = nn.ModuleList()
        self.critic_heads = nn.ModuleList()
        self.ns_ls_ls = []
        self.na_ls_ls = []
        self.n_n_ls = []

        # Initialize Separate Attention Layers for Each Agent
        self.attention_layers = nn.ModuleList()

        for i in range(self.n_agent):
            n_n, n_ns, n_na, ns_ls, na_ls = self._get_neighbor_dim(i)
            self.ns_ls_ls.append(ns_ls)
            self.na_ls_ls.append(na_ls)
            self.n_n_ls.append(n_n)
            self._init_comm_layer(n_n, n_ns, n_na)
            n_a = self.n_a if self.identical else self.n_a_ls[i]
            self._init_actor_head(n_a)
            self._init_critic_head(n_na)

            # Initialize the self-attention layer for each agent
            attention_layer = nn.MultiheadAttention(
                embed_dim=self.n_h, num_heads=1)
            init_layer(attention_layer, 'fc')
            self.attention_layers.append(attention_layer)

    def _reset(self):
        self.states_fw = torch.zeros(self.n_agent, self.n_h * 2)
        self.states_bw = torch.zeros(self.n_agent, self.n_h * 2)

    def _run_actor_heads(self, hs, detach=False):
        ps = []
        for i in range(self.n_agent):
            if detach:
                p_i = F.softmax(self.actor_heads[i](
                    hs[i]), dim=1).squeeze().cpu().detach().numpy()
            else:
                p_i = F.log_softmax(self.actor_heads[i](hs[i]), dim=1)
            ps.append(p_i)
        return ps
####################################################################################################  KEY
###ÈÄôÂÄãfunctionÂ∞±ÊòØËôïÁêÜ‰∫ÜÊâÄÊúâagentÁöÑËº∏ÂÖ•ÔºåËº∏Âá∫ÊòØLSTM outputÔºåÊâÄ‰ª•ÈÄôÂÄãfunctionÁöÑ‰∏ã‰∏ÄÊ≠•Â∞±ÊòØÈÄ≤ÂÖ•actor headË∑ücritic head ‰∫Ü„ÄÇÂú®forward‰∏≠ÔºåÂ∞±ÊòØÂÖàÂëºÂè´run_comm_layersÔºåÁÑ∂ÂæåÂ∞±ÈÄ≤ÂÖ•_run_actor_headsË∑ü_run_critic_head‰∫ÜÔºÅÔºÅ
# fps Â≠ò‰∫ÜÂÖ®ÈÉ®agent Âú®t-1ÊôÇÁöÑpolicy(ÈùûactionÂî∑ÔºÅ)
# p ÂâáÊòØÂæûfps‰∏≠ÈÅ∏Âá∫Áï∂ÂâçagentÈÑ∞Â±ÖÁöÑpolicy

####################################################################################################
# _run_comm_layers Function
#
# Purpose:
# This function processes the inputs for all agents at each time step, updates their LSTM states 
# (hidden and cell states), and applies self-attention to the updated hidden states. The output 
# of this function is the LSTM hidden states after being processed by self-attention.
#
# Inputs:
# - obs   : Tensor of shape (T, n_agent, n_s) - Observations for all agents over T time steps.
# - dones : Tensor of shape (T, n_agent)      - Done flags indicating if an episode ended for each agent.
# - fps   : Tensor of shape (T, n_agent, n_a) - Policy fingerprints (previous policies) for all agents.
# - states: Tensor of shape (n_agent, 2 * n_h) - Combined hidden and cell states for all agents.
#
# Outputs:
# - outputs: Tensor of shape (T, n_agent, n_h) - The LSTM outputs (hidden states) for all agents over T time steps,
#            after applying self-attention.
# - new_states: Tensor of shape (n_agent, 2 * n_h) - The updated hidden and cell states for all agents.
#
# Next Steps:
# The output of this function (LSTM outputs) is passed into the actor head and critic head to generate:
# - The action probabilities (policy) via the actor head.
# - The value estimation via the critic head.
####################################################################################################
 
    def _run_comm_layers(self, obs, dones, fps, states):
        
        obs = batch_to_seq(obs) #agnetÁ¨¨‰∏ÄÈ†ÖËº∏ÂÖ• ###all map's observation
        dones = batch_to_seq(dones)
        fps = batch_to_seq(fps) #agentÁ¨¨‰∏âÈ†ÖËº∏ÂÖ• ### all agent's output policy

        #agentÁ¨¨‰∫åÈ†ÖËº∏ÂÖ•
        h, c = torch.chunk(states, 2, dim=1)  ###stateÊòØÂâç‰∏ÄÊôÇÂàªLSTMÁöÑHidden State (h)+Cell State (c)‰∏≤Êé•Ëµ∑‰æÜÔºåËÆäÊàê‰∏ÄÁ∂≠ÂêëÈáè„ÄÇ
        h = h.to(self.device)   #short-term memory or output
        c = c.to(self.device)   #long-term memory
        outputs = []

        for t, (x, p, done) in enumerate(zip(obs, fps, dones)): ##For each time step t, process the observations, policy fingerprints, and done flags.
            ##Ê≥®ÊÑèÔºÅÂú®ÈÄÅÂÖ•get_comm_s‰πãÂâçÔºåÊâÄÊúâÁöÑobs,fps,h,cÈÉΩÊòØÊâÄÊúâagentÔºåÊâÄÊúâÂçÅÂ≠óË∑ØÂè£Á∂úÂêàËµ∑‰æÜ‰∏ÄËµ∑Â≠òÂú®‰∏ÄÂÄãlistË£°ÁöÑÔºÅÊòØÊääÁ¨¨i‰ΩçagentÈÄÅÂÖ•get_comm_sÂæåÔºåÁ¨¨i‰ΩçagentÊâçÂéªÂÖ®ÈÉ®ÁöÑx(obs),p(fps),(h,c)Êâæ‰ªñËá™Â∑±ÈÑ∞Â±ÖÁöÑ‰æÜÁî®ÔºÅ
            done = done.to(self.device)
            x = x.to(self.device)
            p = p.to(self.device)
            #Prepare Lists to Store Updated Hidden and Cell States(LSTM output)
            next_h = []  
            next_c = []
            x = x.squeeze(0)
            p = p.squeeze(0)
        ## Process each agent separately
            for i in range(self.n_agent):
                n_n = self.n_n_ls[i]
                if n_n:
                    ####get_comm_sË£°Èù¢Êúâ‰∏âÂÄãFCÔºåÂÖ∂ÂØ¶ÂêàÁêÜÔºåÂõ†ÁÇ∫ _run_comm_layersÊòØÂú®forward Ë£°Ë¢´ÂëºÂè´ÁöÑÔºåÊâÄ‰ª•Âú®‰∏ãÈù¢ÈÄôÈÉ®ÔºåÂ∞±ÊòØÁ¨¨‰∏ÄÊ¨°ÁúüÊ≠£ÊääËº∏ÂÖ•ÊµÅÈÅéNN
                    s_i = self._get_comm_s(i, n_n, x, h, p)   ##return s_i„ÄÇÁï∂Ââçagent number(i)„ÄÅn_n(number of neighbors)„ÄÅx(all agent's observation),h(all LSTM's hidden state),p(all agents policy)‰∏üÈÄ≤Âéªget_comm_sÔºå‰ªñÂ∞±ÊúÉËá™ÂãïÁØ©ÈÅ∏Âá∫ÈÑ∞Â±ÖÁöÑË≥áË®ä„ÄÇÂæóÂà∞Á¨¶ÂêàÊ≠§‰Ωçagent(agent i)ÁöÑÂ∞àÂ±¨s_i
                else:
                    if self.identical:
                        x_i = x[i].unsqueeze(0)
                    else:
                        x_i = x[i].narrow(0, 0, self.n_s_ls[i]).unsqueeze(0)
                    s_i = F.relu(self.fc_x_layers[i](x_i))
                ####Âü∑Ë°åÂà∞ÈÄôË£°ÔºåÂ∞±Ê∫ñÂÇôÊääs_iÈ§µÂÖ•LSTM‰∫ÜÔºÅ
                '''
                LSTM Forward Pass
At each time step, the LSTM takes:

1.Input (x_t): The current input (feature vector).
2.Previous Hidden State (h_t-1): The hidden state from the previous time step.
3.Previous Cell State (c_t-1): The cell state from the previous time step.

The LSTM updates these states and produces:

1.New Hidden State (h_t): The output of the LSTM at the current time step.(Èô§‰∫ÜÊòØshort term outputÔºå‰πüÊòØÈ†êÊ∏¨ÁöÑÁõÆÊ®ôoutput)
2.New Cell State (c_t): The updated long-term memory.
                '''
                ##Êï¥ÁêÜ‰∏Ä‰∏ãË¶ÅÈ§µÂÖ•LSTMÁöÑÊù±Ë•ø
                ##1. s_iÔºöÁ∂ìÈÅéFCËôïÁêÜÂæåÁöÑÊâÄÊúâagentËÉΩËßÄÂØüÂà∞ÁöÑË®äÊÅØ
                ##2. h[i] : current hidden  states for agent i
                ##3. c[i] : current cell    states for agent i
                ##unsqueeze(0)Ôºö adds an extra dimension to make their shape (1, n_h) (suitable for LSTM input).
                ####LSTM Â±§Ê†πÊìöÁï∂ÂâçÁöÑËº∏ÂÖ• s_i ÂíåÂÖàÂâçÁöÑÈö±ËóèÁãÄÊÖã(h_i)‰ª•ÂèäÁ¥∞ËÉûÁãÄÊÖã(c_i)„ÄÇ‰æÜË®àÁÆóÊñ∞ÁöÑÈö±ËóèÁãÄÊÖã next_h_i ÂíåÁ¥∞ËÉûÁãÄÊÖã next_c_i 
                h_i, c_i = h[i].unsqueeze(0) * (1 - done), c[i].unsqueeze(0) * (1 - done)
                ###Ëº∏ÂÖ•ÊµÅÈÅéÁöÑÁ¨¨‰∫åÂÄãNN(LSTM)
                ###LSTMÁöÑËº∏Âá∫Ë´ãÁúã Ôºö https://chatgpt.com/share/675b22d9-4400-8013-8102-1a28d8c13ffe
                '''
                Key Context to Differentiate
                lstm_out, (h_n, c_n) = self.lstm(x)   # lstm_out contains hidden states for all time steps: shape (batch_size, seq_length, hidden_size)

                Êé•‰∏ã‰æÜÂàÜÁÇ∫ÂÖ©Á®Æ‰ΩøÁî®LSTM outputÁöÑÊñπÂºè
                Sequence-to-One: If after running this LSTM operation, you only take the last hidden state (next_h_i[-1]) to pass into a fully connected layer for a single prediction.(e,gÊúÄÂæå‰∏ÄÂ§©ËÇ°ÂÉπ)
                if we want to get single value prediction for the entire sequence:
                final_hidden_state = lstm_out[:, -1, :] 

                
                Sequence-to-Sequence: If you collect all hidden states (next_h_i at each time step) and produce outputs for each step.(e.g.ÊñáÁ´†ÊØèÂÄãÂ≠óË©ûÊÄßÂàÜÊûê)
                # in init Ôºö self.fc = nn.Linear(hidden_size, output_size)
                out = self.fc(lstm_out)          # Fully connected layer applied to each time step
                return out
                https://chatgpt.com/share/675b22d9-4400-8013-8102-1a28d8c13ffe
                '''
                next_h_i, next_c_i = self.lstm_layers[i](s_i, (h_i, c_i))  #next_h_iÔºöÊõ¥Êñ∞ÂæåÁöÑÈö±ËóèÁãÄÊÖã(Êñ∞ÁöÑshort term memoryÔºÅÔºåÂêåÊôÇ‰πüÊòØLSTMÁöÑfinal outputÔºÅ)ÔºåÂ∞áÂú®‰∏ã‰∏ÄÊ≠•‰ΩúÁÇ∫ Self-Attention ÁöÑËº∏ÂÖ•„ÄÇ
                next_h.append(next_h_i)#ÂõûÊÉ≥LSTMÁöÑÂúñÁâáÔºåÈÄôÂ∞±Á≠âÊñºÊòØÈï∑Èï∑ÁöÑshort term memoryÂæÄÂè≥ÈÇäÂèàÂ§ö‰∫Ü‰∏ÄÁ≠ÜË≥áÊñô„ÄÇÂæÄÂè≥ÈÇäÂ§öÊñ∞Â¢û‰∫Ü‰∏ÄÂÄãLSTM block
                next_c.append(next_c_i)#Èï∑Èï∑ÁöÑlong term memoryÂæÄÂè≥ÈÇäÂèàÂ§ö‰∫Ü‰∏ÄÁ≠ÜË≥áÊñô
            # after LSTM cell updates
            h = torch.cat(next_h) # shape: (n_agent, n_h)
            c = torch.cat(next_c)

            # Apply each agent's self-attention layer to its hidden state
            # Apply self-attention layer for each agent
            h_seq = h.unsqueeze(1)  # (n_agent, n_h) --> (n_agent, 1, n_h)
            attn_output_list = []
            for i in range(self.n_agent):
                attn_output, _ = self.attention_layers[i](h_seq[i].unsqueeze(0), h_seq[i].unsqueeze(0), h_seq[i].unsqueeze(0))
                attn_output_list.append(attn_output.squeeze(0))
            h = torch.cat(attn_output_list, dim=0)
            outputs.append(h.unsqueeze(0))


            # Concatenate all agents' attention outputs
            h = torch.stack(attn_output_list)  # Shape: (n_agent, n_h)
            outputs.append(h.unsqueeze(0))    # Shape: (1, n_agent, n_h)

    
        outputs = torch.cat(outputs)           # Shape: (batch, n_agent, n_h)
        return outputs, torch.cat([h, c], dim=1)

    def _run_critic_heads(self, hs, actions, detach=False):
        vs = []
        for i in range(self.n_agent):
            n_n = self.n_n_ls[i]
            if n_n:
                js = torch.from_numpy(
                    np.where(self.neighbor_mask[i])[0]).long()
                na_i = torch.index_select(actions, 0, js)
                na_i_ls = []
                for j in range(n_n):
                    na_i_ls.append(
                        one_hot(na_i[j], self.na_ls_ls[i][j]).cuda())
                h_i = torch.cat([hs[i]] + na_i_ls, dim=1)
            else:
                h_i = hs[i]
            v_i = self.critic_heads[i](h_i).squeeze()
            if detach:
                vs.append(v_i.cpu().detach().numpy())
            else:
                vs.append(v_i)
        return vs

#####


class NCLMMultiAgentPolicy(NCMultiAgentPolicy):
    def __init__(self, n_s, n_a, n_agent, n_step, neighbor_mask, n_fc=64, n_h=64,
                 n_s_ls=None, n_a_ls=None, groups=0, identical=True):
        Policy.__init__(self, n_a, n_s, n_step, 'nclm', None, identical)
        if not self.identical:
            self.n_s_ls = n_s_ls
            self.n_a_ls = n_a_ls
        self.n_agent = n_agent
        self.neighbor_mask = neighbor_mask
        self.groups = groups
        self.n_fc = n_fc
        self.n_h = n_h
        self._init_net()
        self._reset()

    def backward(self, obs, fps, acts, dones, Rs, Advs,
                 e_coef, v_coef, summary_writer=None, global_step=None):
        obs = torch.from_numpy(obs).float().transpose(0, 1)
        dones = torch.from_numpy(dones).float()
        fps = torch.from_numpy(fps).float().transpose(0, 1)
        acts = torch.from_numpy(acts).long()
        hs, new_states = self._run_comm_layers(obs, dones, fps, self.states_bw)
        # backward grad is limited to the minibatch
        self.states_bw = new_states.detach()
        ps = self._run_actor_heads(hs)
        bps = self._run_actor_heads(hs, acts)
        # for i in range(self.n_agent):
        #     if i % 2 != 0:
        #         ps[i] = bps[i]
        for i in range(self.n_agent):
            if i in self.groups:
                ps[i] = bps[i]

        vs = self._run_critic_heads(hs, acts)
        self.policy_loss = 0
        self.value_loss = 0
        self.entropy_loss = 0
        Rs = torch.from_numpy(Rs).float()
        Advs = torch.from_numpy(Advs).float()
        for i in range(self.n_agent):
            actor_dist_i = torch.distributions.categorical.Categorical(
                logits=ps[i])
            policy_loss_i, value_loss_i, entropy_loss_i = \
                self._run_loss(actor_dist_i, e_coef, v_coef, vs[i],
                               acts[i], Rs[i], Advs[i])
            self.policy_loss += policy_loss_i
            self.value_loss += value_loss_i
            self.entropy_loss += entropy_loss_i
        self.loss = self.policy_loss + self.value_loss + self.entropy_loss
        self.loss.backward()
        if summary_writer is not None:
            self._update_tensorboard(summary_writer, global_step)

    def forward(self, ob, done, fp, action=None, out_type='p'):
        # TxNxm
        ob = torch.from_numpy(np.expand_dims(ob, axis=0)).float()
        done = torch.from_numpy(np.expand_dims(done, axis=0)).float()
        fp = torch.from_numpy(np.expand_dims(fp, axis=0)).float()
        # h dim: NxTxm
        h, new_states = self._run_comm_layers(ob, done, fp, self.states_fw)
        if out_type.startswith('p'):
            self.states_fw = new_states.detach()
            if (np.array(action) != None).all():
                action = torch.from_numpy(
                    np.expand_dims(action, axis=1)).long()
            return self._run_actor_heads(h, action, detach=True)
        else:
            action = torch.from_numpy(np.expand_dims(action, axis=1)).long()
            return self._run_critic_heads(h, action, detach=True)

    def _init_comm_layer(self, n_n, n_ns, n_na):
        n_lstm_in = 3 * self.n_fc
        # fc_x_layer Êú¨Âú∞ËßÄÊ∏¨ÂÄºÂæóÂÖ®ÈÄ£Êé•Â±§
        fc_x_layer = nn.Linear(n_ns, self.n_fc)

        init_layer(fc_x_layer, 'fc')
        self.fc_x_layers.append(fc_x_layer)
        if n_n:
            # fc_p_layer ÈÑ∞Â±ÖÁöÑÁ≠ñÁï•ÊåáÁ¥ã (Â∞±ÊòØt-1ÊôÇÈÑ∞Â±ÖÁöÑpolicy)
            fc_p_layer = nn.Linear(n_na, self.n_fc)
            init_layer(fc_p_layer, 'fc')
            # fc_m_layer ÈÑ∞Â±ÖÁöÑË®äÊÅØÊåáÁ¥ã
            fc_m_layer = nn.Linear(self.n_h * n_n, self.n_fc)
            init_layer(fc_m_layer, 'fc')
            self.fc_m_layers.append(fc_m_layer)
            self.fc_p_layers.append(fc_p_layer)
            lstm_layer = nn.LSTMCell(n_lstm_in, self.n_h)
        else:
            self.fc_m_layers.append(None)
            self.fc_p_layers.append(None)
            lstm_layer = nn.LSTMCell(self.n_fc, self.n_h)
        init_layer(lstm_layer, 'lstm')
        self.lstm_layers.append(lstm_layer)

    def _init_backhand_actor_head(self, n_a, n_na):
        # only discrete control is supported for now
        actor_head = nn.Linear(self.n_h + n_na, n_a)
        init_layer(actor_head, 'fc')
        self.actor_heads.append(actor_head)

    def _init_net(self):
        self.fc_x_layers = nn.ModuleList()
        self.fc_p_layers = nn.ModuleList()
        self.fc_m_layers = nn.ModuleList()
        self.lstm_layers = nn.ModuleList()
        self.actor_heads = nn.ModuleList()
        self.critic_heads = nn.ModuleList()
        self.ns_ls_ls = []
        self.na_ls_ls = []
        self.n_n_ls = []
        for i in range(self.n_agent):
            n_n, n_ns, n_na, ns_ls, na_ls = self._get_neighbor_dim(i)
            self.ns_ls_ls.append(ns_ls)
            self.na_ls_ls.append(na_ls)
            self.n_n_ls.append(n_n)
            self._init_comm_layer(n_n, n_ns, n_na)
            n_a = self.n_a if self.identical else self.n_a_ls[i]
            if i not in self.groups:
                # first move
                self._init_actor_head(n_a)
            else:
                self._init_backhand_actor_head(n_a, n_na)
            self._init_critic_head(n_na)

    def _run_actor_heads(self, hs, preactions=None, detach=False):
        ps = [0] * self.n_agent
        if (np.array(preactions) == None).all():
            for i in range(self.n_agent):
                if i not in self.groups:  # first hand
                    if detach:
                        p_i = F.softmax(self.actor_heads[i](
                            hs[i]), dim=1).cpu().squeeze().detach().numpy()
                    else:
                        p_i = F.log_softmax(self.actor_heads[i](hs[i]), dim=1)
                    ps[i] = p_i
        else:
            for i in range(self.n_agent):
                if i in self.groups:  # back hand
                    n_n = self.n_n_ls[i]
                    if n_n:
                        js = torch.from_numpy(
                            np.where(self.neighbor_mask[i])[0]).long()
                        na_i = torch.index_select(preactions, 0, js)
                        na_i_ls = []
                        for j in range(n_n):
                            na_i_ls.append(
                                one_hot(na_i[j], self.na_ls_ls[i][j]).cuda())
                        h_i = torch.cat([hs[i]] + na_i_ls, dim=1)
                    else:
                        h_i = hs[i]
                    if detach:
                        p_i = F.softmax(self.actor_heads[i](
                            h_i), dim=1).cpu().squeeze().detach().numpy()
                    else:
                        p_i = F.log_softmax(self.actor_heads[i](h_i), dim=1)
                    ps[i] = p_i
        return ps
